{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1b17b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import json\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import vaex\n",
    "# import mpl_scatter_density\n",
    "# import matplotlib as mpl\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "# mpl.style.use('seaborn-colorblind')\n",
    "# mpl.rc('font', size=15)\n",
    "# mpl.rc('figure', figsize=(8, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54707d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/scratch/08317/tg876168/ananke_subsamples/m12i/lsr-0/lsr-0-rslice-0.m12i-res7100-subsamples.hdf5',\n",
       " '/scratch/08317/tg876168/ananke_subsamples/m12i/lsr-0/lsr-0-rslice-1.m12i-res7100-subsamples.hdf5',\n",
       " '/scratch/08317/tg876168/ananke_subsamples/m12i/lsr-0/lsr-0-rslice-2.m12i-res7100-subsamples.hdf5']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_dir = '/scratch/08317/tg876168/ananke_subsamples/m12i/lsr-0/'\n",
    "sim_files = sorted(glob.glob(os.path.join(sim_dir, '*.hdf5')))\n",
    "sim_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7db49fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b', 'dec', 'feh', 'l', 'labels', 'parallax', 'parallax_over_error', 'pmdec', 'pmra', 'px_true', 'py_true', 'pz_true', 'ra', 'radial_velocity', 'source_id', 'vx_true', 'vy_true', 'vz_true']\n",
      "50683177\n",
      "['b', 'dec', 'feh', 'l', 'labels', 'parallax', 'parallax_over_error', 'pmdec', 'pmra', 'px_true', 'py_true', 'pz_true', 'ra', 'radial_velocity', 'source_id', 'vx_true', 'vy_true', 'vz_true']\n",
      "13581\n",
      "['b', 'dec', 'feh', 'l', 'labels', 'parallax', 'parallax_over_error', 'pmdec', 'pmra', 'px_true', 'py_true', 'pz_true', 'ra', 'radial_velocity', 'source_id', 'vx_true', 'vy_true', 'vz_true']\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i_file in range(len(sim_files)):\n",
    "    with h5py.File(sim_files[i_file], 'r') as f:\n",
    "        print(list(f.keys()))\n",
    "        print(f['ra'].len())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15c5591a",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ('l', 'b', 'parallax', 'pmra', 'pmdec', 'radial_velocity','labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c47b03",
   "metadata": {},
   "source": [
    "### Split data into train, validation, and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0728e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed1 = 5642556\n",
    "seed2 = 7196865\n",
    "\n",
    "n_max_file = 10000    # 1M samples per file\n",
    "# n_max_file = 10000000    # 10M samples per file\n",
    "\n",
    "out_dir_base = '/scratch/08317/tg876168/dataset/m12i-lsr-0-test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1acb1fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in keys:\n",
    "    full_data = []\n",
    "    for sim_file in sim_files:\n",
    "        with h5py.File(sim_file, 'r') as input_f:\n",
    "            full_data.append(input_f[p][:])\n",
    "    full_data = np.concatenate(full_data)\n",
    "    \n",
    "    # split dataset into training and validation\n",
    "    train_data, val_data = train_test_split(\n",
    "        full_data, test_size=0.2, random_state=seed1, shuffle=True)\n",
    "    val_data, test_data = train_test_split(\n",
    "        val_data, test_size=0.5, random_state=seed2, shuffle=True)\n",
    "    \n",
    "    # write each dataset\n",
    "    for data, flag in zip(\n",
    "        (train_data, val_data, test_data), ('train', 'val', 'test')):\n",
    "        n_file = data.shape[0] // n_max_file + 1\n",
    "        \n",
    "        out_dir = os.path.join(out_dir_base, flag)     \n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        \n",
    "        for i in range(n_file):\n",
    "            file = os.path.join(out_dir, 'n{:02d}.hdf5'.format(i))\n",
    "            with h5py.File(file, 'a') as output_f:\n",
    "                # write sliced dataset to file\n",
    "                start = i * n_max_file\n",
    "                end = (i + 1) * n_max_file\n",
    "                output_f.create_dataset(p, data=data[start: end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "431e622a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "Number of in situ samples   : 40245442\n",
      "Number of accreted samples  : 311966\n",
      "Fraction of in situ samples : 0.9923\n",
      "Fraction of accreted samples: 0.0077\n",
      "Ratio accreted-in situ      : 0.0078\n",
      "---------------\n",
      "val\n",
      "Number of in situ samples   : 5030947\n",
      "Number of accreted samples  : 38729\n",
      "Fraction of in situ samples : 0.9924\n",
      "Fraction of accreted samples: 0.0076\n",
      "Ratio accreted-in situ      : 0.0077\n",
      "---------------\n",
      "test\n",
      "Number of in situ samples   : 5030735\n",
      "Number of accreted samples  : 38941\n",
      "Fraction of in situ samples : 0.9923\n",
      "Fraction of accreted samples: 0.0077\n",
      "Ratio accreted-in situ      : 0.0077\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "# print out the counts and fraction of in situ and accreted stars for each flag\n",
    "# we want to make sure each flag is drawn from the same distribution\n",
    "\n",
    "properties = {}\n",
    "\n",
    "for flag in ('train', 'val', 'test'):\n",
    "    files = sorted(glob.glob(os.path.join(out_dir_base, f'{flag}/*')))\n",
    "\n",
    "    labels = []\n",
    "    for file in files:\n",
    "        with h5py.File(file, 'r') as f:\n",
    "            labels.append(f['labels'][:])\n",
    "    labels = np.concatenate(labels)\n",
    " \n",
    "    n_insitu = np.sum(labels == 0)\n",
    "    n_accreted = np.sum(labels == 1)\n",
    "    n_total = len(labels)\n",
    "    \n",
    "    print(flag)\n",
    "    print('Number of in situ samples   : {:d}'.format(n_insitu))\n",
    "    print('Number of accreted samples  : {:d}'.format(n_accreted))\n",
    "    print('Fraction of in situ samples : {:.4f}'.format(n_insitu / n_total))\n",
    "    print('Fraction of accreted samples: {:.4f}'.format(n_accreted / n_total))\n",
    "    print('Ratio accreted-in situ      : {:.4f}'.format(n_accreted / n_insitu))\n",
    "    print('---------------')\n",
    "    \n",
    "    properties[flag] = {\n",
    "        'n_insitu': int(n_insitu),\n",
    "        'n_accreted': int(n_accreted),\n",
    "        'n_total': int(n_total),\n",
    "        'f_insitu': float(n_insitu / n_total),\n",
    "        'f_accreted': float(n_accreted / n_total),\n",
    "        'r_accreted_insitu': float(n_accreted / n_insitu)\n",
    "    }\n",
    "\n",
    "with open(os.path.join(out_dir_base, 'properties.json'), 'w') as f:\n",
    "    json.dump(properties, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9af1cc7",
   "metadata": {},
   "source": [
    "### Preprocessing dict\n",
    "During training, we will scale the training data such that for each feature, the mean and standard deviation is 0 and 1, respectively. We want to compute these values prior to training and store it somewhere such that it can be read easily (and quickly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6745e71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: l\n",
      "Mean: 3.8857e-02\n",
      "stdv: 9.2767e+01\n",
      "----------------\n",
      "Keys: b\n",
      "Mean: 8.0702e-01\n",
      "stdv: 3.1606e+01\n",
      "----------------\n",
      "Keys: parallax\n",
      "Mean: 1.0820e+00\n",
      "stdv: 1.0426e+00\n",
      "----------------\n",
      "Keys: pmra\n",
      "Mean: -9.8260e-01\n",
      "stdv: 2.2359e+01\n",
      "----------------\n",
      "Keys: pmdec\n",
      "Mean: -2.7594e+00\n",
      "stdv: 2.2913e+01\n",
      "----------------\n",
      "Keys: radial_velocity\n",
      "Mean: -3.5690e+00\n",
      "stdv: 6.8684e+01\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "train_files = sorted(glob.glob(os.path.join(out_dir_base, 'train/*')))\n",
    "\n",
    "preprocess_dict = {}\n",
    "\n",
    "# loop over all keys and compute mean and stdv\n",
    "for p in keys:\n",
    "    if p == 'labels':\n",
    "        continue\n",
    "    data = []\n",
    "    for file in train_files:\n",
    "        with h5py.File(file, 'r') as f:\n",
    "            data.append(f[p][:])\n",
    "    data = np.concatenate(data)\n",
    "    \n",
    "    mean = np.nanmean(data)\n",
    "    stdv = np.nanstd(data)\n",
    "    \n",
    "    print(f'Keys: {p}')\n",
    "    print('Mean: {:.4e}'.format(mean))\n",
    "    print('stdv: {:.4e}'.format(stdv))\n",
    "    print('----------------')\n",
    "    \n",
    "    preprocess_dict[p] = {\n",
    "        'mean': float(mean), \n",
    "        'stdv': float(stdv),\n",
    "    }\n",
    "    \n",
    "with open(os.path.join(out_dir_base, 'preprocess.json'), 'w') as f:\n",
    "    json.dump(preprocess_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac450216",
   "metadata": {},
   "source": [
    "### shuffle test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71c8a23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dee1c538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the labels\n",
    "train_files = sorted(glob.glob(os.path.join(out_dir_base, 'train/*')))\n",
    "with h5py.File(train_files[0], 'r') as f:\n",
    "    labels = f['labels'][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23be25a7",
   "metadata": {},
   "source": [
    "Naive batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cf40827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2472\n",
      "3468\n",
      "4705\n",
      "5369\n",
      "7188\n",
      "7885\n",
      "8223\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "data_loader = DataLoader(labels, batch_size=batch_size)\n",
    "for i, y in enumerate(data_loader):\n",
    "    n0 = torch.sum(y == 0).numpy()\n",
    "    n1 = torch.sum(y == 1).numpy()\n",
    "    if n1 == 0:\n",
    "        print(f'{i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763289de",
   "metadata": {},
   "source": [
    "Weighted batching ensures the same number of data of each class in each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c552f68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_sample_count = np.array([len(np.where(labels==t)[0]) for t in np.unique(labels)])\n",
    "weight = 1. / class_sample_count\n",
    "samples_weight = np.array([weight[t] for t in labels])\n",
    "\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "sampler = torch.utils.data.sampler.WeightedRandomSampler(\n",
    "    samples_weight.type('torch.DoubleTensor'), len(samples_weight))\n",
    "\n",
    "data_loader = DataLoader(labels, batch_size=batch_size, sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "948123b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(495) tensor(505)\n"
     ]
    }
   ],
   "source": [
    "for y in data_loader:\n",
    "    n0 = torch.sum(y == 0)\n",
    "    n1 = torch.sum(y == 1)\n",
    "    print(n0, n1)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39194375",
   "metadata": {},
   "source": [
    "### Dataset test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ea9d6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e06548fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(out_dir_base, 'preprocess.json')) as f:\n",
    "    preprocess_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c9b9c6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import h5py\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch.utils.data\n",
    "\n",
    "def sort_key(string):\n",
    "    base = os.path.basename(string)\n",
    "    base = os.path.splitext(base)[0]\n",
    "    return int(base.split('_')[-1][1:])\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    ''' Customized Dataset object for large dataset with multiple HDF5 files.\n",
    "        Iterate over the input directory and read in only one HDF5 file at once to save memory.\n",
    "        Can be passed directly into torch.utils.data.DataLoader.\n",
    "    '''\n",
    "    def __init__(self, input_dir, input_key, label_key=None, \n",
    "                 transform=None, shuffle=False, n_max_file=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        if isinstance(input_key, str):\n",
    "            input_key = [input_key, ]\n",
    "\n",
    "        # set attributes\n",
    "        self.input_dir = input_dir\n",
    "        self.input_key = input_key\n",
    "        self.label_key = label_key\n",
    "        self.shuffle = shuffle\n",
    "        self.transform = transform\n",
    "\n",
    "        self.input_dim = len(input_key)\n",
    "    \n",
    "        # get a list of file in directory\n",
    "        self.input_files = sorted(\n",
    "            glob.glob(os.path.join(input_dir, 'n*.hdf5')), key=sort_key)[:n_max_file]\n",
    "        self.__check_keys(input_key)\n",
    "        self.__check_keys(label_key)\n",
    "        \n",
    "        self.sizes = self.__get_sizes()\n",
    "        self.sizes_c = np.cumsum(self.sizes)\n",
    "        \n",
    "        # set data cache\n",
    "        self.cache = [None, None]\n",
    "        self.current_file_index = None\n",
    "    \n",
    "    def __check_keys(self, key):\n",
    "        ''' Iterate through self.input_files and check if keys exist '''\n",
    "        if key is None:\n",
    "            return\n",
    "        \n",
    "        for file in self.input_files:\n",
    "            with h5py.File(file, 'r') as f:\n",
    "                if isinstance(key, str):\n",
    "                    if f.get(key) is None:\n",
    "                        raise KeyError('Key {} does not exist in file {}'.format(key, file))\n",
    "                else:\n",
    "                    for k in key:\n",
    "                        if f.get(k) is None:\n",
    "                            raise KeyError('Key {} does not exist in file {}'.format(k, file))\n",
    "      \n",
    "    def __get_sizes(self):\n",
    "        ''' Iterate through self.input_files and count the \n",
    "        number of samples in each file '''\n",
    "        \n",
    "        sizes = []\n",
    "        for file in self.input_files:\n",
    "            with h5py.File(file, 'r') as f:\n",
    "                sizes.append(f[self.input_key[0]].len())\n",
    "        return sizes\n",
    "    \n",
    "    def __len__(self):\n",
    "        ''' Return number of samples '''\n",
    "        return np.sum(self.sizes)\n",
    "        \n",
    "    def __cache(self, file_index):\n",
    "        ''' Cache data '''\n",
    "        # reset cache data\n",
    "        self.cache = [None, None]\n",
    "        \n",
    "        with h5py.File(self.input_files[file_index], 'r') as f:\n",
    "            # read in input data\n",
    "            input_data = []\n",
    "            for key in self.input_key:\n",
    "                input_data.append(f[key][:])\n",
    "            input_data = np.stack(input_data, 1)\n",
    "                \n",
    "            if self.shuffle:\n",
    "                rand = np.random.permutation(input_data.shape[0])\n",
    "                input_data = input_data[rand]\n",
    "            self.cache[0] = input_data\n",
    "            \n",
    "            # read in label data\n",
    "            if self.label_key is not None:\n",
    "                label = f[self.label_key][:].reshape(-1, 1)\n",
    "                if self.shuffle:\n",
    "                    label = label[rand]\n",
    "                self.cache[1] = label\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        ''' Get item of a given index. Index continuous between files \n",
    "        in the same order of self.input_files '''\n",
    "        \n",
    "        file_index = np.digitize(index, self.sizes_c)\n",
    "        if file_index != self.current_file_index:\n",
    "            self.__cache(file_index)\n",
    "            self.current_file_index = file_index\n",
    "        if file_index != 0:\n",
    "            index -= self.sizes_c[file_index-1]\n",
    "          \n",
    "        if self.transform is not None:\n",
    "            input_data = self.transform(self.cache[0][index])\n",
    "        else:\n",
    "            input_data = self.cache[0][index]\n",
    "        \n",
    "        if self.label_key is not None:\n",
    "            labels = self.cache[1][index]\n",
    "            return input_data, labels\n",
    "        return input_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6ff6663e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f2daaff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_key = ('l', 'b')\n",
    "mean = [preprocess_dict[k]['mean'] for k in input_key]\n",
    "std = [preprocess_dict[k]['stdv'] for k in input_key]\n",
    "\n",
    "def transform(x):\n",
    "    return (x - mean) / std\n",
    "# transform = transforms.Compose([\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=mean, std=std)\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "train_dir = os.path.join(out_dir_base, 'train')\n",
    "dataset = Dataset(\n",
    "    train_dir, input_key, label_key='labels', transform=transform,\n",
    "    shuffle=False, n_max_file=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2c904bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "607a5180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.36792046, -0.35372509]), array([0]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "50b2a46b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 2), (10000, 1))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.cache[0].shape, dataset.cache[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298740b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
